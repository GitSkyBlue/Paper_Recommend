from langchain_community.vectorstores import FAISS
from langchain.schema import Document
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain_huggingface import HuggingFaceEmbeddings
from dotenv import load_dotenv
import os
from openai import OpenAI

load_dotenv()

def SetData(json_Data):
    results = []

    for data in json_Data:
        temp = {}
        temp['paperId'] = data['paperId']
        temp['title'] = data['title']
        temp['abstract'] = data['abstract']

        results.append(temp)

    return results

def CheckSimilarity(search_Query, json_Data):
    # âœ… ë°ì´í„° (title + abstract ê²°í•©)
    try:
        documents = [
            Document(
                page_content=f"{item['title']}\n\n{item['abstract'] or ''}", 
                metadata={"paperId": item["paperId"], "title": item["title"]}
            )
            for item in json_Data
        ]

        # âœ… ì„ë² ë”© ëª¨ë¸
        embeddingsModel = HuggingFaceEmbeddings(model_name='sentence-transformers/msmarco-distilbert-dot-v5')

        # âœ… FAISS ì¸ë±ìŠ¤ ìƒì„±
        retriever = FAISS.from_documents(documents, embeddingsModel).as_retriever(search_kwargs={'k': 10})

        # âœ… Reranker ëª¨ë¸
        model = HuggingFaceCrossEncoder(model_name='BAAI/bge-reranker-v2-m3')
        compressor = CrossEncoderReranker(model=model, top_n=5)

        # âœ… ì••ì¶• ê²€ìƒ‰ê¸° (Reranker ì ìš©)
        compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)

        compressed_docs = compression_retriever.invoke(search_Query)
        # print('='*100, compressed_docs)
        return compressed_docs
    except:
        print('qwer')

############################################################
from fastapi import APIRouter, Query
from .models import SimilarityRequest
import json

router = APIRouter()

# ì „ì—­ ë³€ìˆ˜
embeddings_model = None
reranker_model = None
compressor = None

@router.on_event("startup")
def load_models():
    global embeddings_model, reranker_model, compressor
    print("ğŸ”§ ëª¨ë¸ ë¡œë”© ì¤‘...")
    embeddings_model = HuggingFaceEmbeddings(model_name='sentence-transformers/msmarco-distilbert-dot-v5')
    reranker_model = HuggingFaceCrossEncoder(model_name='BAAI/bge-reranker-v2-m3')
    compressor = CrossEncoderReranker(model=reranker_model, top_n=5)
    print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

# âœ… POST ë°©ì‹ API
@router.post("/CheckSimilarity")
def check_similarity(request: SimilarityRequest):
    try:
        # âœ… Document ë³€í™˜
        documents = [
            Document(
                page_content=f"{item.title}\n\n{item.abstract or ''}",
                metadata={"paperId": item.paperId, "title": item.title}
            )
            for item in request.json_data
        ]

        # âœ… FAISS ì¸ë±ìŠ¤ ìƒì„±
        retriever = FAISS.from_documents(documents, embeddings_model).as_retriever(search_kwargs={'k': 10})

        # âœ… ì••ì¶• ê²€ìƒ‰ê¸° (Reranker ì ìš©)
        compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)

        compressed_docs = compression_retriever.invoke(request.search_query)
        # print(compressed_docs)
        return compressed_docs

    except Exception as e:
        print("âŒ ì—ëŸ¬ ë°œìƒ:", e)
        return {"error": "ìœ ì‚¬ë„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}

    
# if __name__ == '__main__':
#     client = OpenAI()
#     query = 'Attention is all you needì— ëŒ€í•´ì„œ ì•Œê³ ì‹¶ì–´ìš”'

#     search_Query, user_Request = search.KeywordAndTranslate(query, client)
#     print(search_Query, user_Request)

#     json_Data = search.FindBySearchQuery(search_Query)

#     results = SetData(json_Data)

#     sim = CheckSimilarity(results)
#     for data in sim:
#         print(data.page_content)