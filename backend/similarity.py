from langchain_community.vectorstores import FAISS
from langchain.schema import Document
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain_huggingface import HuggingFaceEmbeddings
from dotenv import load_dotenv
from fastapi import APIRouter
from .models import SimilarityRequest

load_dotenv()

router = APIRouter()

# ì „ì—­ ë³€ìˆ˜
embeddings_model = None
reranker_model = None
compressor = None

@router.on_event("startup")
def load_models():
    global embeddings_model, reranker_model, compressor
    print("ğŸ”§ ëª¨ë¸ ë¡œë”© ì¤‘...")
    embeddings_model = HuggingFaceEmbeddings(model_name='sentence-transformers/msmarco-distilbert-dot-v5')
    reranker_model = HuggingFaceCrossEncoder(model_name='BAAI/bge-reranker-v2-m3')
    compressor = CrossEncoderReranker(model=reranker_model, top_n=5)
    print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

# âœ… POST ë°©ì‹ API
@router.post("/CheckSimilarity")
def check_similarity(request: SimilarityRequest):
    try:
        # âœ… Document ë³€í™˜
        documents = [
            Document(
                page_content=f"{item.title}\n\n{item.abstract or ''}",
                metadata={"paperId": item.paperId, "title": item.title}
            )
            for item in request.json_data
        ]

        # âœ… FAISS ì¸ë±ìŠ¤ ìƒì„±
        retriever = FAISS.from_documents(documents, embeddings_model).as_retriever(search_kwargs={'k': 10})

        # âœ… ì••ì¶• ê²€ìƒ‰ê¸° (Reranker ì ìš©)
        compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)

        compressed_docs = compression_retriever.invoke(request.search_query)
        # print(compressed_docs)
        return compressed_docs

    except Exception as e:
        print("âŒ ì—ëŸ¬ ë°œìƒ:", e)
        return {"error": "ìœ ì‚¬ë„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}

    
if __name__ == '__main__':
    pass